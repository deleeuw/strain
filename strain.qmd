---
title: Nonmetric Strain
author: 
    - name: Jan de Leeuw
      orcid: 0000-0003-1420-1797
      email: jan@deleeuwpdx.net
      affiliation: 
        - name: University of California Los Angeles
          city: Los Angeles
          state: CA
          url: www.ucla.edu
      license: "CC0"
date: last-modified
date-format: long
bibliography: [mypubs.bib, total.bib]
number-sections: true
pdf-engine: lualatex
keep-tex: true
format:
   pdf:
    fontsize: 12pt
    include-in-header: 
     - preamble.tex
    keep-tex: true
    link-citations: true
    documentclass: scrartcl
    number-sections: true
   html:
    fontsize: 12pt
    include-in-header: 
     - preamble.css
    keep-md: true
    number-sections: true
toc: true
toc-depth: 3
editor: source
papersize: letter
graphics: true
link-citations: true
mainfont: Times New Roman
abstract: TBD
---

```{r loadpackages, echo = FALSE}
suppressPackageStartupMessages(library(knitr, quietly = TRUE))
suppressPackageStartupMessages(library(tinytex, quietly = TRUE))
```

```{r data, echo = FALSE}
source("munsellData.R")
source("morseData.R")
source("strainSSMissing.R")
source("procrustus.R")
```

\sectionbreak 

**Note:** This is a working manuscript which will be expanded/updated
frequently. All suggestions for improvement are welcome. All Rmd, tex,
html, pdf, R, and C files are in the public domain. Attribution will be
appreciated, but is not required. The files can be found at
<https://github.com/deleeuw/strain> 

\sectionbreak

# Introduction

In Mutidimensional Scaling (MDS) the strain loss function is defined as
\begin{equation}
\sigma(X,\Delta):=\frac14\text{tr}\ \{J(\Delta^2-D^2(X))J\}^2\label{eq-strain}
\end{equation}
Here $J$ is the centering matrix of order $n$, i.e. $J:=I-n^{-1}E$, with $E$ a matrix with all elements equal to one. Matrix $\Delta^2$ has squared dissimilarities and $D^2(X)$ contains the squared Euclidean distances between the rows of an $n\times p$ column-centered configuration matrix $X$.

We will also use another representation of strain in this paper. By carrying out the double-centering we find
\begin{equation}
\sigma(X,\Delta)=\text{tr}\ (C(\Delta)- XX')^2.\label{eq-strainc}
\end{equation}
with
\begin{equation}
C(\Delta):=-\frac12 J\Delta^2J.\label{eq-c}
\end{equation}

In non-metric MDS we have to minimize strain over both $X$ and $\Delta$, where $\Delta$ varies in some subset of the space $\mathcal{D}$ of non-negative, symmetric and hollow matrices of order $n$. In @trosset_98 there is a nice overview of previous attempts to minimize non-metric strain in MDS. 

The simplest, and oldest, form of non-metric scaling estimates the additive
constant. Thus $\delta_{ij}=\delta_{ij}^0+\theta$ for all $i\not= j$, where
$\delta_{ij}^0$ are the given dissimilarities. Only one additional parameter
needs to be estimated, with the side-condition that the adjusted $\delta_{ij}$
are non-negative. The next non-metric scaling problem occurs when
some of the $\delta_{ij}$ are missing, either by accident or by design.
Thus
$$
\delta_{ij}=\begin{cases}\delta_{ij}^0&\text{ for non-missing},\\
\theta_{ij}&\text{ for missing}.
\end{cases}
$$
Thus the number of additional parameters is equal to the number of
missing elements. We will require the $\theta_{ij}$ to be
non-negative. An important speciasl case of the missig data
problem is the unfolding problem, in which we have two sets
of objects. The between-set distances are known, while both 
matrices of within-set distances are missing.

A third important form of non-metric MDS is
ordinal MDS. We require the $\delta_{ij}$ to non-negative and monotone with
the $\delta_{ij}^0$. More precisely if $\delta_{ij}^0\geq\delta_{kl}^0$
then we must have $\delta_{ij}\geq\delta_{kl}\geq 0$.

For the additive constant problem and the missing data problem the
constraint sets are convex polyhedrons that do not contain the 
origin, but for ordinal MDS $\Delta=0$ satisfies the ordinal
constraints. Choosing $X=0$ and $\Delta=0$ produces $\sigma=0$
which is clearly the global minimum. To prevent this trivial
solution from happening we use a normalization condition on
either $X$ or $\Delta$ thate keeps both of them away from 
the origin.

\sectionbreak

# Alternating Least Squares 

In our technique for strain minimization we apply alternating least squares (ALS)
to loss function \eqref{eq-strain}. We start with some initial estimate $\smash{\Delta^{(0)}}$. Then, in each iteration, there are two subproblems. We first minimize strain
over $X$ with $\Delta$ fixed at its current value, and then minimize over $\Delta$
in $\mathcal{D}$ with $X$ fixed at its current value, which is the optimal value from the first subproblem in the iteration. Then we test for convergence, and if the loss function still decreases we go to the next iteration. 

In iteration $k$ this means
\begin{subequations}
\begin{align}
X^{(k)}&=\mathop{\text{argmin}}_X\sigma(X,\Delta^{(k-1)}),\\
\Delta^{(k)}&=\mathop{\text{argmin}}_{\Delta\in\mathcal{D}}\sigma(X^{(k)},\Delta)
\end{align}
\end{subequations}
If a subproblem in an iteration is too complicated for some reason, then we
merely make a step towards its minimum, making sure that
\begin{equation}
\sigma(X^{(k)},\Delta^{(k)})<\sigma(X^{(k)},\Delta^{(k-1)})<\sigma(X^{(k-1)},\Delta^{(k-1)}).
\end{equation}

In the first subproblem minimizing strain over $X$ for fixed $\Delta$ is done by computing the diagonal matrix with $p$ dominant eigenvalues $\Lambda_p$ of $C$ and the corresponding eigenvalues $K_p$. Fir this we use the R package RSpectra (@qiu_mei_24). If  $\Lambda_p\geq 0$ then the optimal $X$ is $\smash{K_p\Lambda_p^\frac12}$. If some elements of $\Lambda_p$ are negative, we first replace them by zeroes. This is classical metric
multidimensional scaling (@torgerson_58, @gower_66, @mardia_78). The fact that computing the classical scaling solution is equivalent to minimizing the loss function \eqref{eq-strain} was first pointed out by @deleeuw_heiser_C_82.

The second subproblem, minimizing strain of $\Delta$ for fixed $X$ (and thus fixed $D$), can take various forms. We discuss some special cases, and their implementation, below.

## The Additive Constant

If a constant $\theta$ is added to the off-diagonal dissimilarities the disparities are
\begin{align}
\Delta^2&=\Delta_0^2+2\theta\Delta_0+\theta^2(E-I).\label{eq-addone}
\end{align}
Because we want the $\delta_{ij}$ to be non-negative we require that $\theta\geq-\min\Delta_0$, where the minimum is taken over all off-diagonal elements of $\Delta$.


From \eqref{eq-addone} we have, with $G:=J\Delta_0J$ and $H:=J(\Delta_0^2-D^2)J$, 
\begin{equation}
\sigma(\theta):=\frac14\text{tr}\ \left\{J(\Delta_0^2+2\theta\Delta_0+\theta^2(E-I)-D^2)J\right\}^2=\frac14\text{tr}\ \left\{H+2\theta G-\theta^2 J\right\}^2,\label{eq-straint2}
\end{equation}
which is a quartic in $\theta$. Expanding and simplifying gives
\begin{equation}
\sigma(\theta)=\frac14\left\{\text{tr}\ H^2+4\theta\text{tr}\ HG+2\theta^2(\text{tr}\ 2G^2-\text{tr}\ H)-4\theta^3\text{tr}\ G+\theta^4(n-1)\right\}.\label{eq-poly}
\end{equation}
We minimizing the quartic over the half-open interval $\theta\geq-\min\Delta$ using the formulas in @jeffrey_97. We implemented Jeffrey's solution in the C function jeffrey(), which is in the shared library jeffrey.so. The R program strainSSAddOne() loads the shared library and 
iteratively executes the two ALS sub problems.

Our example is taken from @torgerson_58, pages 280-290. Dissimilarty judgments of nine Munsell
colors of the same red hue, but differing in brightness and saturation, where collected from 38 subjects using the method of triads. The matrix $\Delta_0$, which Torgerson calls the
"comparative distances" is taken from his Table 5 on p 286.
```{r munselldata, echo = FALSE}
print(munsellMatrix)
```
Torgerson (p, 286-287) uses an elegant, but elaborate, method to find an additive constant value of 3.60. His technique is not based on minimization of an explicit loss function. 
Iterating strainSSAddOne() until strain changes less than `r 1e-10` from one iteration to the 
next, starting with $\theta=0$ uses 196 iterations and find $\theta=2.85$. Starting with
Torgerson's 3.6 value for the additive constant gives the same $\theta$ in 184 iterations.
Of course the `r 1e-10` stopping criterion is much too strict for most applications, although
a run with even higher precision shows the value for the optimal additive constant only has three stable decimals. Using a more realistic `r 1e-3` requires 105 iterations and gives two stable decimals. In other words, convergence is slow. 

## Imputing Missing Data

If there are missing data the classical scaling method to minimize strain must be adapted. In
that case there is a set $\mathcal{M}$ of $m$ index pairs that code for missing data. Matrix
$\Delta_0^2$ now has zeroes for missing data and squared dissimilarities for the non-missing ones. We have
\begin{equation}
\Delta^2=\Delta_0^2+\sum_{(i,j)\in\mathcal{M}}\theta_{(i,j)}E_{(i,j)},
\end{equation}
where $E_{(i,j)}:=e_ie_j'+e_je_i'$ and $e_i$ has element $i$ equal to one and all other elements zero. If $D^2$ are the current squared distances, then strain is the function of $\theta$ defined as
\begin{equation}
\sigma(\theta):=\text{tr}\ \{H-\sum_{(i,j)\in\mathcal{M}}\theta_{(i,j)\in\mathcal{M}}JE_{(i,j)\in\mathcal{M}}J)\}^2,
\label{eq-strmis}
\end{equation}
where $H$ is as defined before.

Minimizing $\sigma$ from \eqref{eq-strmis} over the $m$ element vector $\theta\geq 0$ is a non-negative linear least squares problem
that we solve by using the R function nnls() from the package with the same name (@mullen_vanstokkum_24). The R function strainSSMissing() calls nnls(), as well as eigs_sym
from RSpectra, to solve the two ALS subproblems.

We give two examples. The first uses the classical Morse code data from @rothkopf_57. The data are
confusion probabilities between 36 auditory Morse code signals, collected from 598 airmen.
For details of the design we refer to Rothkopf. The data are available, for example, in the smacof package (@deleeuw_mair_A_09c). With 36 objects there are 630 dissimilarities. We ran the
strainSSMissing() function with 0, 5, 10, 50, and 100 random dissimilarities missing. The five configurations are matched with procrustus() from procrustus.R, which minimizes $\sum_k\|Z-X_kM_k\|^2$ over $Z$ and the square orthonormal $M_k$.

```{r missing, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 8, cache = FALSE} 
h100 <- strainSSMissing(misData(morseData, 100), verbose = FALSE)
h50 <- strainSSMissing(misData(morseData, 50), verbose = FALSE)
h10 <- strainSSMissing(misData(morseData, 10), verbose = FALSE)
h5 <- strainSSMissing(misData(morseData, 5), verbose = FALSE)
h0 <- strainSSMissing(misData(morseData, 0), verbose = FALSE)
h <- list(h0$conf, h5$conf, h10$conf, h50$conf, h100$conf)
h <- procrustus(h, itmax = 1000, eps = 1e-10, verbose = FALSE)
par(pty="s")
plot(h$x[[1]], type = "n", xlab = "dim 1", ylab = "dim 2", main = "Morse data, various numbers of missing")
text(h$x[[1]], "1")
text(h$x[[2]], "2")
text(h$x[[3]], "3")
text(h$x[[4]], "4")
text(h$x[[5]], "5")
text(h$z, "z", col = "RED", cex =2)
```
The plot shows a stable solution. Points are labeled "0" for no missing,
"1" for 5 missing, and so on. The red points are for the matched $Z$
matrix. The number of iterations for 0, 5, 10, 50, 100 missing data is
`r c(h0$itel, h5$itel, h10$itel, h50$itel, h100$itel)` and
strain is `r c(h0$loss, h5$loss, h10$loss, h50$loss, h100$loss)`.

roskam



## Ordinal MDS

For fixed $D$ strain is a function of the disparities $\Delta$, which are required to be monotone with $\Delta_0$. Thus
\begin{equation}
\sigma(\Delta)=\frac14\text{tr}\ J(\Delta - D^2)J(\Delta - D^2)J\label{eq-straindel}
\end{equation}
Note that we fit disparities $\Delta$ to squared distances.
By using the vec of our matrices $\Delta$ and $D$ strain can also be written in the computationally more friendly form
\begin{equation}
\sigma(\delta)=\frac14(\delta - d^2)(J\otimes J)(\delta - d^2)\label{eq-strainvec},
\end{equation}
with $J\otimes J$ the Kronecker product.

Equation  \eqref{eq-strainvec} shows that minimizing strain over $\delta$ is a weighted monotone regression problem. It is more complicated
than standard monotone regression because of the weights $J\otimes J$, which
are not diagonal. Thus strain as a function of $\delta$, although convex, is not separable. In addition $J\otimes J$ is singular, having $(n-1)^2$ unit eigenvalues and $2n-1$ zero eigenvalues. Routines such as PAVA cannot be used, although active set methods are still a viable alternative (@deleeuw_hornik_mair_A_09). 

We will go an alternative route, using majorization to reduce the weighted least squares problem to a sequence of unweighted problems, for which we can use ordinary monotone regression. Related results on majorizing weighted by unweighted least squares are in @kiers_97 and @groenen_giaquinto_kiers_03.

Suppose $\tilde\Delta$ is the current best value of the disparities. 
Expand strain around $\tilde\Delta$.
\begin{equation}
\sigma(\Delta)=\sigma(\tilde\Delta)+\frac12\text{tr}\ J(\Delta-\tilde\Delta)J(\tilde\Delta-D^2)+\frac14\text{tr}\ J(\Delta-\tilde\Delta)J(\Delta-\tilde\Delta), \label{eq-strexpand}
\end{equation}
Because $J\otimes J\lesssim I$ we have
$\sigma(\Delta)\leq\eta(\Delta,\tilde\Delta)$,
with the majorization function $\eta$ defined as
\begin{equation}
\eta(\Delta,\tilde\Delta):=\sigma(\tilde\Delta) +\frac12\text{tr}\  J(\Delta-\tilde\Delta)J(\tilde\Delta-D^2) +\frac14\text{tr}\ (\Delta-\tilde\Delta)^2.\label{eq-etadef}
\end{equation}
The majorization, or MM, algorithm minimizes $\eta$ over vectors $\Delta$ in the polyhedral convex cone of vectors withnincresing elements to find the update $\Delta^+$.

To simplify the expression for $\eta$ we "complete the square". Relying heavily
on the idempotency of $J$, and thus of $J\otimes J$, we find
\begin{equation}
\eta(\Delta,\tilde\Delta):=\frac14\text{tr}\ (\Delta-\hat\Delta)^2,\label{eq-etasimp}
\end{equation}
with
\begin{equation}
\hat\Delta:=\tilde\Delta-J(\tilde\Delta-D^2)J.\label{hdeltadef}
\end{equation}
Thus $\hat\Delta$ is a matrix-weighted average of the “conservative” part $\tilde\Delta$ and the “progressive” part $D^2$ (Chamberlain and Leamer (1976)).

Finally the update $\Delta^+$ is the least squares projection of $\hat\Delta$ on the cone, i.e. the unweighted monotone regression on $\hat\Delta$. We now have
the sandwich inequality
\begin{equation}
\sigma(\Delta^+)\leq\eta(\Delta^+,\tilde\Delta)\leq\eta(\tilde\Delta,\tilde\Delta)=\sigma(\tilde\Delta).\label{eq-sandwich}
\end{equation}
In the substep of ALS in which we improve $\Delta$ we make a number of "inner" majorization iterations. In the later "outer" ALS iterations we will already have a good initial estimate to start these "inner" iterations.

## Bound Constraints

Bound constraints are of the form $\Delta_-\leq\Delta\leq\Delta_+$, where
the inequality signs refer to elementwise comparisons. Thus each $\delta_{ij}$
is required to be in a given interval (which may be infinitely large). We use the same majorization results as in the previous section for updating $\Delta$, i.e. we project each element $\hat\Delta$ from \eqref{eq-hdeltadef} on the corresponding interval. Each update
$\delta^+_{ij}$ is thus equal to either $\hat\delta_{ij}$ or to the closest
end-point of its interval. By choosing the interval to be the non-negative
real axis this gives us another way, using majorization, to impute missing data.


\sectionbreak

# Software

\sectionbreak

# Discussion

Most MDS loss functions are of the form
\begin{equation}
\sigma(X):=\sum_k w_k(f(\delta_k)-f(d_k(X)))^2\label{eq-fstress}
\end{equation}
for some given function $f$. @groenen_deleeuw_mathar_C_95 call loss function \eqref{eq-fstress} fStress. For $f$ equal to the identity we have stress (@kruskal_64a), for $f$ the square we have sstress (@takane_young_deleeuw_A_77), and for $f$ the logarithm we have log-stress (@ramsay_77). 
If we compare \eqref{eq-fstress} and strain we see that strain is more complicated because of the non-diagonal Kronecker weights. This makes the usual optimal transformations in MDS, such as monotone regression in the ordinal case, more complicated and costly. As a consequence non-metric MDS techniques minimizing strain have not been implemented and used to the same degree as techniques based on variations of fStress. 

On the other hand in fStress the minimization over $X$ for given $\Delta$ is
more costly than it is for strain, in which we simply have to find the
$p$ largest eigenvalues and corresponding eigenvectors of a matrix. Thus,
the ALS framework, for fStress finding optimal $\Delta$ for fixed $x$ is
cheap and finding optimal $X$ for fixed $\Delta$ is expensive. For strain
it is exactly the other way around. To put it even more sharply, in
fStress finding the optimal $X$ is impossible, because it requires an
inifinite iterative process which can converge to a local minimum. For strain,
although solving the eigen-problem is also an infinite iterative
process, it quickly gets us as close as we want to the global minimum (for
given $Delta$, that is).

@trosset_02

@trosset_98 discusses an implementation or ordinal MDS uing strain. His
aproach is quite different from ours, because it does not use ALS but
gradient projection. More importantly, Trosset projects out the 
optimization over $X$. He calls this "variable reduction", to contrast it with "variable alternation". Thus his strain is a function of $\Delta$ only,
defined as
\begin{equation}
\sigma(\Delta)=\min_X\text{tr}\ (C(\Delta)- XX')^2=\sum_{r=p+1}^n\lambda_r^2(C_+(\Delta)),
\label{eq-varred}
\end{equation}
where the $\lambda_r$ are the $n-p$ smallest eigenvalues of its argument, and with $C_+$ the positive definite part of $C$. This loss function is then
minimized over $\Delta$ by gradient projection, constrained by the linear inequalities that impose monotonicity. It is unclear how this compares to ALS,
although we can say that in general projecting out a set of unknowns tends to
accelerate convergence, at the cost of more complicated subproblems in each
iteration. Also, unlike ALS, gradient projection implies choosing a step-size procedure to guarantee convergence. Projection seems especially suitable for the additive constant problem in which
\eqref{eq-varred} is a function of a single parameter, which can be minimized in a number of ways. 

weighted strain

\sectionbreak

# References
